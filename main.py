from contextlib import asynccontextmanager
from typing import Any

import cv2 as cv
import keras
import numpy as np
from fastapi import FastAPI, Response, UploadFile
from pydantic import BaseModel, RootModel, model_validator

ml_models = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Load the ML model
    ml_models["box_classifier"] = keras.saving.load_model("model.keras")
    yield
    # Clean up the ML models and release the resources
    ml_models.clear()


app = FastAPI(lifespan=lifespan)


class Boxes(RootModel):
    root: list[tuple[int, int, int, int]] = [(1, 1, 1, 1), (1, 1, 1, 1)]

    def __len__(self):
        return len(self.root)


class Labels(RootModel):
    root: list[str] = ["empty", "confirmed"]

    def __len__(self):
        return len(self.root)


class AnnotatedBoxes(BaseModel):
    classes: Labels
    boxes: Boxes

    @model_validator(mode="after")
    def lengths_match(self):
        if len(self.classes) != len(self.boxes):
            raise ValueError("Length of classes is not the same as length of the boxes.")
        return self


# Função para calcular a distância entre dois pontos
def distancia(pt1, pt2):
    return np.sqrt((pt1[0] - pt2[0]) ** 2 + (pt1[1] - pt2[1]) ** 2)


def get_bounding_boxes(img: np.ndarray, template: np.ndarray, threshold: float) -> list:
    w, h = template.shape[::-1]

    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    res = cv.matchTemplate(img_gray, template, cv.TM_CCOEFF_NORMED)
    loc = np.where(res >= threshold)
    boxes = []

    # Processar cada ponto encontrado
    for pt in zip(*loc[::-1]):
        # Verificar se o ponto está muito próximo de algum ponto já processado
        if all(distancia(pt, p) > 100 for p in boxes):  # Limiar de distância = 10 pixels
            box = (pt[0], pt[1], pt[0] + w, pt[1] + h)
            boxes.append(box)
    return boxes


def classify_boxes(
    img: np.ndarray, boxes: list[tuple[int, int, int, int]], model: Any, prediction_threshold: float = 0.9
) -> list[str]:
    response = []
    mapping = {True: "confirmed", False: "empty"}
    for box in boxes:
        box_img = img[box[1] : box[3], box[0] : box[2]]
        box_img = cv.resize(box_img, (224, 224))
        box_img = keras.ops.expand_dims(box_img, axis=0)
        prediction = model.predict(box_img, verbose=0)
        prediction = float(keras.ops.sigmoid(prediction[0][0]))
        response.append(mapping[bool((1 - prediction) >= prediction_threshold)])
    return response


async def read_image(image: UploadFile, flags: int) -> np.ndarray:
    image_contents = await image.read()
    image_arr = np.fromstring(image_contents, np.uint8)
    img = cv.imdecode(image_arr, flags=flags)
    assert img is not None, "file could not be read, check with os.path.exists()"
    return img


# TODO: Como posso colocar n templates?
# TODO: Aplicar a previsão em cima do array de imagens ao inves de prever uma a uma.
# TODO: Retornar a probabilidade de cada classe ao inves de so a classe prevista.


@app.post("/find_boxes/")
async def find_boxes(test_image: UploadFile, box_image: UploadFile, threshold: float = 0.5) -> Boxes:
    img_rgb = await read_image(test_image, flags=cv.IMREAD_COLOR)
    template = await read_image(box_image, flags=cv.IMREAD_GRAYSCALE)

    boxes = get_bounding_boxes(img_rgb, template, threshold)

    return Boxes(root=boxes)


@app.post("/find_answers/")
async def find_answers(
    test_image: UploadFile, box_image: UploadFile, threshold: float = 0.5, prediction_threshold: float = 0.9
) -> AnnotatedBoxes:
    img_rgb = await read_image(test_image, flags=cv.IMREAD_COLOR)
    template = await read_image(box_image, flags=cv.IMREAD_GRAYSCALE)

    boxes = get_bounding_boxes(img_rgb, template, threshold)

    responses = classify_boxes(img_rgb, boxes, ml_models["box_classifier"], prediction_threshold)

    return AnnotatedBoxes(classes=Labels(root=responses), boxes=Boxes(root=boxes))


@app.post(
    "/mark_boxes/",  # Set what the media type will be in the autogenerated OpenAPI specification.
    # fastapi.tiangolo.com/advanced/additional-responses/#additional-media-types-for-the-main-response
    responses={200: {"content": {"image/png": {}}}},
    # Prevent FastAPI from adding "application/json" as an additional
    # response media type in the autogenerated OpenAPI specification.
    # https://github.com/tiangolo/fastapi/issues/3258
    response_class=Response,
)
async def mark_boxes(test_image: UploadFile, box_image: UploadFile, threshold: float = 0.5):
    img_rgb = await read_image(test_image, flags=cv.IMREAD_COLOR)
    template = await read_image(box_image, flags=cv.IMREAD_GRAYSCALE)

    boxes = get_bounding_boxes(img_rgb, template, threshold)

    for box in boxes:
        cv.rectangle(img_rgb, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2)

    _, encoded_img = cv.imencode(".PNG", img_rgb)

    return Response(content=encoded_img.tostring(), media_type="image/png")


@app.post(
    "/mark_answers/",  # Set what the media type will be in the autogenerated OpenAPI specification.
    # fastapi.tiangolo.com/advanced/additional-responses/#additional-media-types-for-the-main-response
    responses={200: {"content": {"image/png": {}}}},
    # Prevent FastAPI from adding "application/json" as an additional
    # response media type in the autogenerated OpenAPI specification.
    # https://github.com/tiangolo/fastapi/issues/3258
    response_class=Response,
)
async def mark_answers(
    test_image: UploadFile, box_image: UploadFile, threshold: float = 0.5, prediction_threshold: float = 0.9
):
    img_rgb = await read_image(test_image, flags=cv.IMREAD_COLOR)
    template = await read_image(box_image, flags=cv.IMREAD_GRAYSCALE)

    boxes = get_bounding_boxes(img_rgb, template, threshold)

    responses = classify_boxes(img_rgb, boxes, ml_models["box_classifier"], prediction_threshold)

    mapping = {"empty": (0, 0, 255), "confirmed": (255, 0, 255)}

    for box, response in zip(boxes, responses):
        color = mapping[response]
        cv.rectangle(img_rgb, (box[0], box[1]), (box[2], box[3]), color, 2)

    _, encoded_img = cv.imencode(".PNG", img_rgb)

    return Response(content=encoded_img.tostring(), media_type="image/png")
